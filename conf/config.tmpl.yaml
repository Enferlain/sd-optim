defaults:
  - _self_
  - payloads: cargo
  - optimisation_guide: guide

run_name: ${merge_mode}_${scorer_method}
hydra:
  run:
    dir: logs/${now:%Y-%m-%d_%H-%M-%S}_${run_name}

url: http://127.0.0.1:7860
webui: a1111  # or "forge"

wildcards_dir: path/to/wildcards/folder
scorer_model_dir: path/to/scorer/models/folder

model_paths:  # Use a list to specify model or lora paths 
  - path/to/model_a.safetensors
  - path/to/model_b.safetensors
  - path/to/model_c.safetensors
  - path/to/model_n.safetensors
base_model_index:  # For methods that require subtract/add_difference (0, 1, 2, ...)

device: cpu # specify device to merge on (cpu, cuda)
threads: 4

merge_mode: weighted_sum # specify merge method https://github.com/ljleb/sd-mecha/blob/main/sd_mecha/merge_methods/__init__.py
model_arch: sdxl  # specify model architecture (sd1, sdxl, sd3, flux)

recipe_optimization:
  enabled: False  # Enable/disable the feature
  recipe_path: path/to/recipe.mecha
  optimization_target:  # Merge node identifier or hyperparameter names

optimizer:
  bayes: True               # Enable Bayesian optimization
  tpe: False                # Enable Tree-structured Parzen Estimator optimization (not implemented)
  atpe: False               # Enable Adaptive TPE optimization (not implemented)

  # Bayesian Optimization Settings:
  random_state: -1           # Seed for optimization run
  bounds_transformer: False  # Reduce the search space during optimization, potentially finding better solutions faster, but might miss the global optimum (bayes only)
  latin_hypercube_sampling: False # Explore the parameter space more evenly, potentially leading to a better starting point for optimization (bayes only)
  load_log_file: None       # Path to the run JSON log file to resume a run (optional)
  reset_log_file: True     # Whether to reset (overwrite) the log file (default: True)

  # Acquisition Function Settings (bayes only):
  acquisition_function:
    kind: ucb              # Type of acquisition function (ucb, ei, poi)
    kappa: 3               # Exploration-exploitation parameter (for ucb)
    xi: 0.05               # Exploration-exploitation parameter (for ei and poi)
    kappa_decay: 0.98      # Kappa decay factor (for ucb)
    kappa_decay_delay: ${optimizer.init_points}

  sampler: sobol        # random, latin_hypercube, sobol, halton
  init_points: 10       # Exploration iterations
  n_iters: 10           # Exploitation iterations

  # Recommendations:
  # - Short runs (1-10 iterations):
  #   - kind: poi
  #   - bounds_transformer: True
  #   - lhs or random sampling for quick results
  #
  # - Low mid (20 iterations):
  #   - bounds_transformer: True
  #   - lhs or Sobol sampling for balanced initial coverage
  #
  # - Medium runs (10-50 iterations):
  #   - kind: ei, xi: 0.05
  #   - bounds_transformer: False/True
  #   - sobol for even distribution in moderate to high dimensions
  #   - halton slightly more randomized, low-discrepancy initial points
  #
  # - Long runs (50+ iterations):
  #   - kind: ucb, kappa: 3.0
  #   - bounds_transformer: True/False based on exploration goal
  #   - sobol for high-dimensional runs where stable, comprehensive coverage is needed
  #
  # Sampling Methods:
  # - Latin Hypercube Sampling (LHS): Generates randomized, well-distributed points within each dimension.
  #   Best for balanced coverage in lower to moderate dimensions (up to ~15 parameters).
  # - Sobol Sampling: Ensures evenly distributed, quasi-random samples for high-dimensional spaces.
  #   Best for 20+ parameters, preventing clustering and under-sampling regions in complex searches.
  # - Halton Sampling: Provides a low-discrepancy, randomized sequence that works well in moderate dimensions.
  #   Ideal for runs that benefit from a bit more stochasticity in exploration.
  #
  # Acquisition Functions:
  # - Probability of Improvement (PI): Favors exploitation, focusing on areas where the model predicts high scores with high confidence.
  # - Expected Improvement (EI): Provides a balance between exploration and exploitation, considering both predicted scores and uncertainty.
  # - Upper Confidence Bound (UCB): Encourages more exploration, focusing on areas with high uncertainty, even if the predicted scores are not the highest.

batch_size: 1     # amount of images per payload
save_imgs: True
img_average_type: arithmetic     # geometric, arithmetic, quadratic

# scorer by type:
# Prompt-Image Alignment:               blip, clip
# Aesthetic:                            chad, laion
# Hybrid(PIA + AES):                    ir, hpsv2, pick
# Anime/Illustration:                   cityaes, shadowv2, cafe, wdaes
# Misc:                                 manual, noai, iqa
#
# !!!! IQA ARE NOT IMPLEMENTED YET !!!!
#
# Notes:
# 1) recommended tested safe setup is [laion, chad, clip, blip, ir] with weights 0.5, 0.5, 1, 1, 1 <- whitewipe's setup
# 2) personal (imi) recommendation is cityaes for anime, also good to filter out broken images and is very fast and small

scorer_method: [cityaes]
scorer_average_type: arithmetic # geometric, arithmetic, quadratic
scorer_weight:
  #blip: 0.5
  #chad: 2
  # example above, default is 1
scorer_default_device: cpu # cuda
scorer_device:
  #blip: cpu
  #chad: cuda
  # example above, default is scorer default device
scorer_alt_location:
  #blip:
    #model_name: scorer.pth
    #model_dir: path/to/scorer/scorer.pth
  #chad:
    #model_name: scorer.pt
    #model_dir: path/to/scorer/scorer.pt
  # example above, default downloads them in the scorer_model_dir(this option is here if you already have them downloaded somewhere else)
scorer_print_individual: True

save_best: True
precision: fp16  # "fp16", "bf16", "fp32", or "fp64"

visualizations:
  scatter_plot: False # not implemented
  unet_diagram: True
  convergence_plot: True
  heatmap: False # not implemented
